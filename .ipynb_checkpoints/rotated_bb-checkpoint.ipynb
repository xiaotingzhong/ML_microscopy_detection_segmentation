{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reference: https://colab.research.google.com/drive/1324pVWKpyscblSNUU1aqSDuDJb6liAvo?usp=sharing#scrollTo=Pd9xjI6TSHT1\n",
    "- data: https://drive.google.com/drive/folders/11XK4OQ9gxcBnQUpD5J6w-xuK7vhynsVz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import detectron2\n",
    "import contextlib\n",
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import copy,torch,torchvision\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as X\n",
    "import math\n",
    "from itertools import repeat\n",
    "\n",
    "from fvcore.common.file_io import PathManager\n",
    "from fvcore.common.timer import Timer\n",
    "\n",
    "from detectron2.structures import Boxes, BoxMode, PolygonMasks\n",
    "from detectron2.config import *\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.evaluation import RotatedCOCOEvaluator,DatasetEvaluators, inference_on_dataset, coco_evaluation,DatasetEvaluator\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import concurrent.futures\n",
    "\n",
    "import torch\n",
    "# torch.cuda.set_device(0)\n",
    "\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications for rotation\n",
    "### Custom mapper\n",
    "https://github.com/facebookresearch/detectron2/issues/21#issuecomment-595522318\n",
    "\n",
    "```utils.transform_instance_annotations``` does not work for rotated boxes and you need a custom version using ```transform.apply_rotated_box```\n",
    "\n",
    "```utils.annotations_to_instances``` needs to be replaced by ```utils.annotations_to_instances_rotated```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_transform_instance_annotations(annotation, transforms, image_size, *, keypoint_hflip_indices=None):\n",
    "    if annotation[\"bbox_mode\"] == BoxMode.XYWHA_ABS:\n",
    "        annotation[\"bbox\"] = transforms.apply_rotated_box(np.asarray([annotation[\"bbox\"]]))[0]\n",
    "    else:\n",
    "        bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n",
    "        # Note that bbox is 1d (per-instance bounding box)\n",
    "        annotation[\"bbox\"] = transforms.apply_box([bbox])[0]\n",
    "        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n",
    "\n",
    "    return annotation\n",
    "\n",
    "def mapper(dataset_dict):\n",
    "    # Implement a mapper, similar to the default DatasetMapper, but with our own customizations\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "    image, transforms = T.apply_transform_gens([T.Resize((800, 800))], image)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "      my_transform_instance_annotations(obj, transforms, image.shape[:2]) \n",
    "      for obj in dataset_dict.pop(\"annotations\")\n",
    "      if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    instances = utils.annotations_to_instances_rotated(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "    return dataset_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        evaluators = [RotatedCOCOEvaluator(dataset_name, cfg, True, output_folder)]\n",
    "        return DatasetEvaluators(evaluators)\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "class RotatedPredictor(DefaultPredictor):\n",
    "    def __init__(self, cfg):\n",
    "        \n",
    "        self.cfg = cfg.clone()  # cfg can be modified by model\n",
    "        self.model = trainer.model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transform_gen = T.ResizeShortestEdge(\n",
    "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "\n",
    "        self.input_format = cfg.INPUT.FORMAT\n",
    "        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n",
    "\n",
    "    def __call__(self, original_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "        Returns:\n",
    "            predictions (dict):\n",
    "                the output of the model for one image only.\n",
    "                See :doc:`/tutorials/models` for details about the format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "            # Apply pre-processing to image.\n",
    "            if self.input_format == \"RGB\":\n",
    "                # whether the model expects BGR inputs or RGB\n",
    "                original_image = original_image[:, :, ::-1]\n",
    "            height, width = original_image.shape[:2]\n",
    "            image = self.transform_gen.get_transform(original_image).apply_image(original_image)\n",
    "            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "            predictions = self.model([inputs])[0]\n",
    "            return predictions\n",
    "\n",
    "# As of 0.3 the XYWHA_ABS box is not supported in the visualizer, this is fixed in master branch atm (19/11/20)\n",
    "class myVisualizer(Visualizer):\n",
    "  \n",
    "    def draw_dataset_dict(self, dic):\n",
    "        annos = dic.get(\"annotations\", None)\n",
    "        if annos:\n",
    "            if \"segmentation\" in annos[0]:\n",
    "                masks = [x[\"segmentation\"] for x in annos]\n",
    "            else:\n",
    "                masks = None\n",
    "            if \"keypoints\" in annos[0]:\n",
    "                keypts = [x[\"keypoints\"] for x in annos]\n",
    "                keypts = np.array(keypts).reshape(len(annos), -1, 3)\n",
    "            else:\n",
    "                keypts = None\n",
    "\n",
    "            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYWHA_ABS) for x in annos]\n",
    "\n",
    "            labels = [x[\"category_id\"] for x in annos]\n",
    "            names = self.metadata.get(\"thing_classes\", None)\n",
    "            if names:\n",
    "                labels = [names[i] for i in labels]\n",
    "            labels = [\n",
    "                \"{}\".format(i) + (\"|crowd\" if a.get(\"iscrowd\", 0) else \"\")\n",
    "                for i, a in zip(labels, annos)\n",
    "            ]\n",
    "            self.overlay_instances(labels=labels, boxes=boxes, masks=masks, keypoints=keypts)\n",
    "\n",
    "        sem_seg = dic.get(\"sem_seg\", None)\n",
    "        if sem_seg is None and \"sem_seg_file_name\" in dic:\n",
    "            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_GRAYSCALE)\n",
    "        if sem_seg is not None:\n",
    "            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle annotations\n",
    "Here we're going to generate the annotations from the dataset. HRSC2016 was written in caffe and has many tools written in C. Therefore they're not natively COCO compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaclasses_simple = {'100000001': 'ship',\n",
    "             '100000002': 'aircraft carrier',\n",
    "             '100000003': 'warcraft',\n",
    "             '100000004': 'merchant ship',\n",
    "             '100000005': 'aircraft carrier',\n",
    "             '100000006': 'aircraft carrier',\n",
    "             '100000007': 'destroyer',\n",
    "             '100000008': 'warcraft',\n",
    "             '100000009': 'destroyer',\n",
    "             '100000010': 'amphibious',\n",
    "             '100000011': 'cruiser',\n",
    "             '100000012': 'aircraft carrier',\n",
    "             '100000013': 'aircraft carrier',\n",
    "             '100000014': 'destroyer',\n",
    "             '100000015': 'amphibious',\n",
    "             '100000016': 'amphibious',\n",
    "             '100000017': 'amphibious',\n",
    "             '100000018': 'merchant ship',\n",
    "             '100000019': 'merchant ship',\n",
    "             '100000020': 'merchant ship',\n",
    "             '100000022': 'Hovercraft',\n",
    "             '100000024': 'ship',\n",
    "             '100000025': 'merchant ship',\n",
    "             '100000026': 'ship',\n",
    "             '100000027': 'submarine',\n",
    "             '100000028': 'ship',\n",
    "             '100000029': 'ship',\n",
    "             '100000030': 'merchant ship',\n",
    "             '100000031': 'aircraft carrier',\n",
    "             '100000032': 'aircraft carrier',\n",
    "             '100000033': 'aircraft carrier'}\n",
    "\n",
    "metaclasses_extensive = {'100000001': 'ship',\n",
    "              '100000002': 'aircraft carrier',\n",
    "              '100000003': 'warcraft',\n",
    "              '100000004': 'merchant ship',\n",
    "              '100000005': 'Nimitz class aircraft carrier',\n",
    "              '100000006': 'Enterprise class aircraft carrier',\n",
    "              '100000007': 'Arleigh Burke class destroyers',\n",
    "              '100000008': 'WhidbeyIsland class landing craft',\n",
    "              '100000009': 'Perry class frigate',\n",
    "              '100000010': 'Sanantonio class amphibious transport dock',\n",
    "              '100000011': 'Ticonderoga class cruiser',\n",
    "              '100000012': 'Kitty Hawk class aircraft carrier',\n",
    "              '100000013': 'Admiral Kuznetsov aircraft carrier',\n",
    "              '100000014': 'Abukuma-class destroyer escort',\n",
    "              '100000015': 'Austen class amphibious transport dock',\n",
    "              '100000016': 'Tarawa-class amphibious assault ship',\n",
    "              '100000017': 'USS Blue Ridge (LCC-19)',\n",
    "              '100000018': 'Container ship',\n",
    "              '100000019': 'OXo|--)',\n",
    "              '100000020': 'Car carrier([]==[])',\n",
    "              '100000022': 'Hovercraft',\n",
    "              '100000024': 'yacht',\n",
    "              '100000025': 'Container ship(_|.--.--|_]=',\n",
    "              '100000026': 'Cruise ship',\n",
    "              '100000027': 'submarine',\n",
    "              '100000028': 'lute',\n",
    "              '100000029': 'Medical ship',\n",
    "              '100000030': 'Car carrier(======|',\n",
    "              '100000031': 'Ford-class aircraft carriers',\n",
    "              '100000032': 'Midway-class aircraft carrier',\n",
    "              '100000033': 'Invincible-class aircraft carrier'}\n",
    "\n",
    "used_classes = metaclasses_simple\n",
    "# used_classes = metaclasses_extensive\n",
    "\n",
    "class_name_list = list(set(used_classes.values())) # list of all unique classes\n",
    "class_id_dict = {} # index of each class by ID\n",
    "for key, name in used_classes.items():\n",
    "    class_id_dict.update({key: class_name_list.index(name)})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_record(file_id, set_folder=\"Train\") :\n",
    "    #open file xml\n",
    "    xml_path = os.path.join(dataset_path, set_folder, 'Annotations', file_id+\".xml\")\n",
    "    img_path = os.path.join(dataset_path, set_folder, 'AllImages', file_id+'.bmp')\n",
    "\n",
    "    # check if Annotation exists and img file exists\n",
    "    if not os.path.isfile(xml_path) or not os.path.isfile(img_path):\n",
    "        return\n",
    "\n",
    "    tree = X.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    record = {}\n",
    "    #store our file data\n",
    "    record[\"file_name\"] = img_path\n",
    "    record[\"image_id\"] = int(root.find(\"Img_FileName\").text)\n",
    "    record[\"height\"] = int(root.find(\"Img_SizeHeight\").text)\n",
    "    record[\"width\"] =  int(root.find(\"Img_SizeWidth\").text)\n",
    "\n",
    "    objs = []\n",
    "    for anno in root.findall(\"HRSC_Objects/HRSC_Object\"):\n",
    "        x1=int(anno.findtext(\"box_xmin\"))\n",
    "        x2=int(anno.findtext(\"box_xmax\"))\n",
    "        y1=int(anno.findtext(\"box_ymin\"))\n",
    "        y2=int(anno.findtext(\"box_ymax\"))\n",
    "        a=-math.degrees(float(anno.findtext(\"mbox_ang\")))\n",
    "        cx=float(anno.findtext(\"mbox_cx\"))\n",
    "        cy=float(anno.findtext(\"mbox_cy\"))\n",
    "        w=float(anno.findtext(\"mbox_w\"))\n",
    "        h=float(anno.findtext(\"mbox_h\"))\n",
    "        obj = {\n",
    "            \"bbox\": [cx,cy,w,h,a],\n",
    "            \"bbox_mode\": BoxMode.XYWHA_ABS,\n",
    "            \"category_id\": class_id_dict[anno.find(\"Class_ID\").text]\n",
    "            }\n",
    "        objs.append(obj)\n",
    "    record[\"annotations\"] = objs\n",
    "    return record\n",
    "\n",
    "def get_dataset(dir):\n",
    "    base_path = os.path.join(dataset_path, dir)\n",
    "    sysdata_path = os.path.join(base_path, 'sysdata.xml')\n",
    "\n",
    "    root = X.parse(sysdata_path).getroot()\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for id in root.findall(\"HRSC_DataSet_Exp/HRSC_DSExpImgs/HRSC_DataSet_Exp_Group/ExpGroup_Imgs/Img_NO\"):\n",
    "        file_id = id.text\n",
    "        record = get_file_record(file_id, dir)\n",
    "        if record is not None:\n",
    "            dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "DatasetCatalog.clear()\n",
    "MetadataCatalog.clear()\n",
    "for d in [\"Train\", \"Test\"]:\n",
    "    DatasetCatalog.register(d, lambda d=d: get_dataset(d))\n",
    "    MetadataCatalog.get(d).set(thing_classes=class_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = get_dataset(\"Train\")\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = myVisualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"Train\"), scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup configuration\n",
    "Now we've got out data in a usable form,and some useful functions lets configure our tests. below are the options for training, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "cfg.OUTPUT_DIR = os.path.join(dataset_path, 'output')\n",
    "\n",
    "#cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\") # Let training initialize from model zoo\n",
    "cfg.DATASETS.TRAIN = ([\"Train\"])\n",
    "cfg.DATASETS.TEST = ([\"Test\"])\n",
    "\n",
    "cfg.MODEL.MASK_ON=False\n",
    "cfg.MODEL.PROPOSAL_GENERATOR.NAME = \"RRPN\"\n",
    "cfg.MODEL.RPN.HEAD_NAME = \"StandardRPNHead\"\n",
    "cfg.MODEL.RPN.BBOX_REG_WEIGHTS = (10,10,5,5,1)\n",
    "cfg.MODEL.ANCHOR_GENERATOR.NAME = \"RotatedAnchorGenerator\"\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ANGLES = [[-90,-60,-30,0,30,60,90]]\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8 \n",
    "cfg.MODEL.ROI_HEADS.NAME = \"RROIHeads\"\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   #this is far lower than usual.  \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES =len(class_name_list)\n",
    "cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE = \"ROIAlignRotated\"\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10,10,5,5,1)\n",
    "cfg.MODEL.ROI_BOX_HEAD.NUM_CONV=4\n",
    "cfg.MODEL.ROI_MASK_HEAD.NUM_CONV=8\n",
    "cfg.SOLVER.IMS_PER_BATCH = 15 #can be up to  24 for a p100 (6 default)\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD=1500\n",
    "cfg.SOLVER.BASE_LR = 0.005\n",
    "cfg.SOLVER.GAMMA=0.5\n",
    "cfg.SOLVER.STEPS=[1000,2000,4000,8000, 12000]\n",
    "cfg.SOLVER.MAX_ITER=14000\n",
    "\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True \n",
    "cfg.DATALOADER.SAMPLER_TRAIN= \"RepeatFactorTrainingSampler\"\n",
    "cfg.DATALOADER.REPEAT_THRESHOLD=0.01\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)#lets just check our output dir exists\n",
    "cfg.MODEL.BACKBONE.FREEZE_AT=6\n",
    "\n",
    "\n",
    "# Notes on how to implement:\n",
    "# https://github.com/facebookresearch/detectron2/issues/21#issuecomment-595522318\n",
    "# MODEL:\n",
    "#   ANCHOR_GENERATOR:\n",
    "#     NAME: RotatedAnchorGenerator\n",
    "#     ANGLES: [[-90,-60,-30,0,30,60,90]]\n",
    "#   PROPOSAL_GENERATOR:\n",
    "#     NAME: RRPN\n",
    "#   RPN:\n",
    "#     BBOX_REG_WEIGHTS: (1,1,1,1,1)\n",
    "#   ROI_BOX_HEAD:\n",
    "#     POOLER_TYPE: ROIAlignRotated\n",
    "#     BBOX_REG_WEIGHTS: (10,10,5,5,1)\n",
    "#   ROI_HEADS:\n",
    "#     NAME: RROIHeads\n",
    "\n",
    "# print(cfg.OUTPUT_DIR)\n",
    "# print(cfg.MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Load tensorboard to view progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"$cfg.OUTPUT_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "trainer = MyTrainer(cfg) \n",
    "trainer.resume_or_load(resume=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The results produced about aren't as accurate as those performed outside of training. So let's rerun our evaluations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_convert_to_coco_dict(dataset_dicts, class_name_list):\n",
    "\n",
    "    coco_images = []\n",
    "    coco_annotations = []\n",
    "    coco_categories = [{\"id\": index, \"name\": value, \"supercategory\": \"empty\"} for\n",
    "                  index, value in enumerate(class_name_list)]\n",
    "\n",
    "    for image_dict in dataset_dicts:\n",
    "        coco_image = {\n",
    "            \"id\": int(image_dict[\"image_id\"]),\n",
    "            \"width\": image_dict[\"width\"],\n",
    "            \"height\": image_dict[\"height\"],\n",
    "            \"file_name\": image_dict[\"file_name\"],\n",
    "        }\n",
    "        coco_images.append(coco_image)\n",
    "\n",
    "        for annotation in image_dict[\"annotations\"]:\n",
    "            coco_annotation = {}\n",
    "\n",
    "            # COCO requirement: XYWH box format\n",
    "            bbox = annotation[\"bbox\"]\n",
    "            bbox_mode = annotation[\"bbox_mode\"]\n",
    "            # Computing areas using bounding boxes\n",
    "            bbox_xy = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n",
    "            area = Boxes([bbox_xy]).area()[0].item()\n",
    "\n",
    "            # COCO requirement:\n",
    "            #   linking annotations to images\n",
    "            #   \"id\" field must start with 1\n",
    "            coco_annotation[\"id\"] = len(coco_annotations) + 1\n",
    "            coco_annotation[\"image_id\"] = coco_image[\"id\"]\n",
    "            coco_annotation[\"bbox\"] = [round(float(x), 3) for x in bbox]\n",
    "            coco_annotation[\"area\"] = float(area)\n",
    "            coco_annotation[\"iscrowd\"] = 0\n",
    "            coco_annotation[\"category_id\"] = annotation[\"category_id\"]\n",
    "\n",
    "            coco_annotations.append(coco_annotation)\n",
    "\n",
    "    info = {\n",
    "        \"date_created\": str(datetime.datetime.now()),\n",
    "        \"description\": \"Automatically generated COCO json file for Detectron2.\",\n",
    "    }\n",
    "\n",
    "    coco_dict = {\n",
    "        \"info\": info,\n",
    "        \"images\": coco_images,\n",
    "        \"annotations\": coco_annotations,\n",
    "        \"categories\": coco_categories,\n",
    "        \"licenses\": None,\n",
    "    }\n",
    "    return coco_dict\n",
    "\n",
    "\n",
    "def my_convert_to_coco_json(output_file, dataset_dicts, class_name_list):\n",
    "    coco_dict = my_convert_to_coco_dict(dataset_dicts, class_name_list)\n",
    "\n",
    "    PathManager.mkdirs(os.path.dirname(output_file))\n",
    "    with PathManager.open(output_file, \"w\") as f:\n",
    "        json.dump(coco_dict, f)\n",
    "\n",
    "coco_json_path = os.path.join(dataset_path, \"Test_coco_format.json\")\n",
    "my_convert_to_coco_json(coco_json_path, get_dataset(\"Test\"), class_name_list)\n",
    "register_coco_instances(\"Test_coco\", {}, coco_json_path, \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug in RotatedCOCOEvaluator where it gets passed img_ids\n",
    "class MyRotatedCOCOEvaluator(RotatedCOCOEvaluator):\n",
    "    def _eval_predictions(self, tasks, predictions, img_ids=None):\n",
    "        super()._eval_predictions(tasks, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create coco evaluator, but use the default detectron2 data format for generation, make sure ids overlap\n",
    "evaluator = MyRotatedCOCOEvaluator(\"Test_coco\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, \"Test\", mapper=mapper) \n",
    "outputs = inference_on_dataset(trainer.model, val_loader, evaluator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "# predictor = RotatedPredictor(cfg)\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "test_dict = get_dataset(\"Test\")\n",
    "for d in random.sample(test_dict, 3):\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = myVisualizer(im[:, :, ::-1],\n",
    "                  metadata=MetadataCatalog.get(\"Test\"), \n",
    "                  scale=0.5)\n",
    "                  # instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    # )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BoxMode.XYWHA_ABS: 4>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoxMode.XYWHA_ABS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2144308 12.285979837613853\n"
     ]
    }
   ],
   "source": [
    "def get_file_record(file_id, dataset_path='/Users/zhong2/Desktop/Data/QuesTek/HRSC2016', set_folder=\"Train\") :\n",
    "    #open file xml\n",
    "    xml_path = os.path.join(dataset_path, set_folder, 'Annotations', file_id+\".xml\")\n",
    "    img_path = os.path.join(dataset_path, set_folder, 'AllImages', file_id+'.bmp')\n",
    "\n",
    "    # check if Annotation exists and img file exists\n",
    "    if not os.path.isfile(xml_path) or not os.path.isfile(img_path):\n",
    "        return\n",
    "\n",
    "    tree = X.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    record = {}\n",
    "    #store our file data\n",
    "    record[\"file_name\"] = img_path\n",
    "    record[\"image_id\"] = int(root.find(\"Img_FileName\").text)\n",
    "    record[\"height\"] = int(root.find(\"Img_SizeHeight\").text)\n",
    "    record[\"width\"] =  int(root.find(\"Img_SizeWidth\").text)\n",
    "\n",
    "    objs = []\n",
    "    for anno in root.findall(\"HRSC_Objects/HRSC_Object\"):\n",
    "        x1=int(anno.findtext(\"box_xmin\"))\n",
    "        x2=int(anno.findtext(\"box_xmax\"))\n",
    "        y1=int(anno.findtext(\"box_ymin\"))\n",
    "        y2=int(anno.findtext(\"box_ymax\"))\n",
    "        a=-math.degrees(float(anno.findtext(\"mbox_ang\")))\n",
    "        cx=float(anno.findtext(\"mbox_cx\"))\n",
    "        cy=float(anno.findtext(\"mbox_cy\"))\n",
    "        w=float(anno.findtext(\"mbox_w\"))\n",
    "        h=float(anno.findtext(\"mbox_h\"))\n",
    "        print(float(anno.findtext(\"mbox_ang\")), a)\n",
    "        obj = {\n",
    "            \"bbox\": [cx,cy,w,h,a],\n",
    "            \"bbox_mode\": BoxMode.XYWHA_ABS,\n",
    "            \"category_id\": class_id_dict[anno.find(\"Class_ID\").text]\n",
    "            }\n",
    "        objs.append(obj)\n",
    "    record[\"annotations\"] = objs\n",
    "    return record\n",
    "\n",
    "a = get_file_record('100000001')\n",
    "# DatasetCatalog.clear()\n",
    "# MetadataCatalog.clear()\n",
    "# for d in [\"Train\", \"Test\"]:\n",
    "#     DatasetCatalog.register(d, lambda d=d: get_dataset(d))\n",
    "#     MetadataCatalog.get(d).set(thing_classes=class_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': '/Users/zhong2/Desktop/Data/QuesTek/HRSC2016/Train/AllImages/100000001.bmp',\n",
       " 'image_id': 100000001,\n",
       " 'height': 753,\n",
       " 'width': 1166,\n",
       " 'annotations': [{'bbox': [582.9349,\n",
       "    353.2006,\n",
       "    778.1303,\n",
       "    174.2541,\n",
       "    12.285979837613853],\n",
       "   'bbox_mode': <BoxMode.XYWHA_ABS: 4>,\n",
       "   'category_id': 6}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
